{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:  ['Real madrid is set to win the UCL for the season .', 'Benzema might win Balon dor .', 'Salah might be the runner up']\n",
      "Word Tokenization:  ['Real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'UCL', 'for', 'the', 'season', '.', 'Benzema', 'might', 'win', 'Balon', 'dor', '.', 'Salah', 'might', 'be', 'the', 'runner', 'up']\n"
     ]
    }
   ],
   "source": [
    "sent_tkn = sent_tokenize(text)\n",
    "word_tkn = word_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokenization: \", sent_tkn)\n",
    "print(\"Word Tokenization: \",word_tkn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS(Part of Speech) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words with tags: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Real', 'JJ'),\n",
       " ('madrid', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('set', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('win', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('UCL', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('season', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Benzema', 'NNP'),\n",
       " ('might', 'MD'),\n",
       " ('win', 'VB'),\n",
       " ('Balon', 'NNP'),\n",
       " ('dor', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Salah', 'NNP'),\n",
       " ('might', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('runner', 'NN'),\n",
       " ('up', 'RP')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokenized words with tags: \")\n",
    "pos_tag(word_tkn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words:  {'isn', 'me', 'hasn', 'hers', 'whom', 'this', 'out', 'haven', 'both', 'my', \"shouldn't\", 'weren', 'ma', 'have', 'between', \"don't\", 'll', 'so', 'yourselves', 'yours', 'he', 'i', 'needn', 'these', 'been', 'at', 'she', \"she's\", 'here', 'from', 'we', 'doing', 'our', 'a', 'did', \"you'll\", 'didn', 'can', 'having', \"mightn't\", 'the', 'doesn', 'what', 'if', 'had', 'are', 'is', 'each', 'aren', 'wasn', 'has', 'shouldn', \"shan't\", 'once', 'm', 'myself', 'as', 've', 'was', 'through', 'after', 'to', 'all', 'how', 'ain', 'were', 'same', \"needn't\", 'below', 'her', 'themselves', 'himself', 'ours', 'ourselves', 'only', 'don', 'too', 'not', 'or', 'further', 'while', 'again', 'over', 'by', \"haven't\", 'his', 'against', 's', 'will', 'which', 'you', \"aren't\", \"weren't\", 'couldn', 'won', 'it', 'then', 'such', \"couldn't\", 'than', 'itself', 'for', 'there', 'mustn', 'shan', 'of', 'with', 'd', \"wasn't\", 'about', 'its', 'hadn', 'their', 'who', \"that'll\", 'herself', 'most', 'nor', 'because', 'why', \"it's\", 'being', \"hadn't\", \"mustn't\", 'on', \"won't\", 'own', 'him', 'those', 'up', 'when', 'theirs', 'mightn', \"you'd\", 'any', 'y', 'more', \"should've\", 'they', 'yourself', 'few', 'them', 'in', 'am', 'but', 'off', 'some', \"you're\", 'down', 'do', 'does', 'and', 'your', \"hasn't\", 're', 'under', 'wouldn', 'an', 'o', \"wouldn't\", 'until', 'into', 'above', 'where', 'very', 't', 'just', \"isn't\", \"you've\", \"didn't\", 'no', 'that', 'be', 'should', 'before', 'other', 'during', \"doesn't\", 'now'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop Words: \",stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real', 'madrid', 'set', 'win', 'UCL', 'season', '.', 'Benzema', 'might', 'win', 'Balon', 'dor', '.', 'Salah', 'might', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# Removal of stopwords\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in word_tkn:\n",
    "    if token.lower() not in stop_words:\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words:  ['better', 'play', 'studi', 'game', 'play', 'the', 'teach', 'play', 'run', 'run', 'code', 'laugh']\n"
     ]
    }
   ],
   "source": [
    "words = [\"better\", \"Playing\", \"Studies\", \"Gaming\", \"Played\", \"the\", \"Teaching\", \"Play\", \"Run\", \"Running\", \"Coding\", \"Laughing\"]\n",
    "\n",
    "stemmed_word = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    stemmed_word.append(ps.stem(word))\n",
    "\n",
    "print(\"Stemmed Words: \", stemmed_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "risks: risk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"risks:\", lemmatizer.lemmatize(\"risks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words:  ['corpus', 'risk', 'study', 'bottle', 'rock', 'the', 'teacher']\n"
     ]
    }
   ],
   "source": [
    "words = [\"corpora\", \"risks\", \"studies\", \"bottles\", \"rocks\", \"the\", \"teachers\"]\n",
    "\n",
    "lemmatized_word = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    lemmatized_word.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "print(\"Lemmatized Words: \", lemmatized_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.4814821314936913\n",
      "  (0, 2)\t0.4814821314936913\n",
      "  (0, 9)\t0.3661795714211074\n",
      "  (0, 4)\t0.3661795714211074\n",
      "  (0, 8)\t0.3661795714211074\n",
      "  (0, 7)\t0.3661795714211074\n",
      "  (1, 11)\t0.5493512310263033\n",
      "  (1, 9)\t0.41779577097245885\n",
      "  (1, 4)\t0.41779577097245885\n",
      "  (1, 8)\t0.41779577097245885\n",
      "  (1, 7)\t0.41779577097245885\n",
      "  (2, 3)\t0.4472135954999579\n",
      "  (2, 10)\t0.4472135954999579\n",
      "  (2, 5)\t0.4472135954999579\n",
      "  (2, 6)\t0.4472135954999579\n",
      "  (2, 1)\t0.4472135954999579\n",
      "\n",
      " ['abhishek' 'agaye' 'and' 'dekhne' 'is' 'ka' 'maut' 'my' 'name' 'saish'\n",
      " 'tamasha' 'why'] \n",
      "\n",
      "\n",
      "abhishek : 0.4814821314936913 \n",
      "agaye : 0.0 \n",
      "and : 0.4814821314936913 \n",
      "dekhne : 0.0 \n",
      "is : 0.3661795714211074 \n",
      "ka : 0.0 \n",
      "maut : 0.0 \n",
      "my : 0.3661795714211074 \n",
      "name : 0.3661795714211074 \n",
      "saish : 0.3661795714211074 \n",
      "tamasha : 0.0 \n",
      "why : 0.0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"My name is saish and Abhishek\", \"why My name is saish\", \"agaye maut ka tamasha dekhne\"]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute TF-IDF values\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\n\", feature_names, \"\\n\\n\") \n",
    "\n",
    "# Display TF-IDF values for each document\n",
    "for i in range(len(feature_names)):\n",
    "  print(\"{} : {} \".format(feature_names[i],tfidf_matrix[0,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
