# Tokenization
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
text='Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up'
sent_tkn = sent_tokenize(text)
word_tkn = word_tokenize(text)

print("Sentence Tokenization: ", sent_tkn)
print("Word Tokenization: ",word_tkn)


# POS(Part of Speech) Tagging
from nltk import pos_tag
print("Tokenized words with tags: ")
pos_tag(word_tkn)


# Stopwords
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
print("Stop Words: ",stop_words)
# Removal of stopwords

filtered_tokens = []
for token in word_tkn:
    if token.lower() not in stop_words:
        filtered_tokens.append(token)

print(filtered_tokens)
# Stemming with NLTK
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
words = ["better", "Playing", "Studies", "Gaming", "Played", "the", "Teaching", "Play", "Run", "Running", "Coding", "Laughing"]

stemmed_word = []

ps = PorterStemmer()

for word in words:
    stemmed_word.append(ps.stem(word))
print("Stemmed Words: ", stemmed_word)


# Lemmatization with NLTK
from nltk.stem import WordNetLemmatizer
 
lemmatizer = WordNetLemmatizer()
 
print("rocks :", lemmatizer.lemmatize("rocks"))
print("corpora :", lemmatizer.lemmatize("corpora"))
print("risks:", lemmatizer.lemmatize("risks"))
words = ["corpora", "risks", "studies", "bottles", "rocks", "the", "teachers"]

lemmatized_word = []

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

for word in words:
    lemmatized_word.append(lemmatizer.lemmatize(word))

print("Lemmatized Words: ", lemmatized_word)


# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample documents
documents = ["My name is saish and Abhishek", "why My name is saish", "agaye maut ka tamasha dekhne"]

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Compute TF-IDF values
tfidf_matrix = vectorizer.fit_transform(documents)
print(tfidf_matrix)

# Get feature names (terms)
feature_names = vectorizer.get_feature_names_out()
print("\n", feature_names, "\n\n") 

# Display TF-IDF values for each document
for i in range(len(feature_names)):
  print("{} : {} ".format(feature_names[i],tfidf_matrix[0,i]))
